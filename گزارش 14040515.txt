گزارش کار
14040515

نظر به اینکه مدت مدیدی است گزارش ندادم سعی می کنم مختصری از آنچه گذشته بیان کنم.

تسکی که به من ارجاع شد این بود که برنامه ای داشته باشیم که بتواند udp packet دریافت کند و آنرا tcp تحویل مقصد دهد.
در ذیل این تسک مشخص شد که قسمت دریافت udp می تواند کند عمل نماید فلذا packet lost اتفاق افتد
پس زیر فعالیتی تعریف شد که بسنجیم چه اتفاقی می افتد که packet lost داریم.
نتیجه ای که حاصل شد این بود که وقتی برنامه ای که نوشته بودم udp می گرفت سعی می کرد بلافاصله آنرا از طریق tcp بفرستد فلذا این وقفه ناچیز زمانی این اشکال را ایجاد می کرد که buffer udp دچار اشکال می شد و دیتای ورودی که شاید بتوان گفت با معیارهای ابزارهای ارسال پکت با سریعترین سرعت ارسال می شد این بافر را اشباع می کرد و در نتیجه دریافت پکت متوقف می شد. پس زیر فعالیت دیگری که تعریف شد این بود که این باتل نک رفع گردد. در نتیجه برای قسمت دریافت udp سه حالت در نظر گرفته شد که یا فقط ورودی را دریافت کند و شمارش کند و یا آنرا به همین صورت بطور متوالی بخواند و بلافاصله برای ارسال آن اقدام کند و حالت سوم اینکه خواندن udp و ارسال tcp در دو thread متفاوت صورت گیرد.
بعد از پیاده سازی این سه حالت و تست عملکرد آنها، در شرایطی که فقط دیتای udp دریافت کنیم و با شرایط اینکه هیچ وقفه ای دیگر در کار نباشد همچنان مشاهده شد که packet lost داریم. پس بر روی این مشکل تمرکز بیشتر انجام شد.
برای بررسی این مشکل لازم بود ببینم آیا ابزارهای دیگری که می توان برای ارسال و دریافت استفاده کرد و آنها هم از کرنل لینوکس استفاده می کنند همین مشکل را دارند و یا خیر.
برای اینکار ابزار های مختلفی بررسی شد و نهایتا به دستورات tcpdump , tshark رسیدم و برای ارسال هم بررسی کردم تولید کننده udp خودم از ابزار های دیگری مانند hping , nc کمک گرفته شد ولی ابزار خودم نرخ بیشتری می توانست پکت ایجاد و ارسال کند.
در نهایت ابزار ارسال کننده که بیشترین نرخ را با کرنل تولید کند ابزار خودم انتخاب شد و benchmark ابزار هایی که دریافت کننده هستند انجام شد که در نهایت مشخص شد روش من برای دریافت udp بسیار بیشتر lost داشت و tshark , tcpdump عملکرد خیلی بهتری داشتند(البته برای وضعیت payload 1 بایت)
در همین تست متوجه شدم اصطلاحا اجرای socket drain عملکرد lost را بهبود می بخشد یعنی ممکن است چند پکت به بافر رسیده باشند و تابع Select که مشخص کننده دریافت است صرفا تغییر وضعیت را گزارش داده است پس اینکار انجام شد ولی همچنان استفاده از تابع recv متوالی نتواست عملکرد ما را به سطح مطلوب تر tshark برساند.
همچنین برای بررسی میزان توان شبکه برای انتقال از ابزار wget استفاده شد که با بررسی بیشتر مشخص شد بدلیل اینکه این روش از packet های با سایز 56k استفاده می کند مقایسه ناصحیحی به نسبت شرایط واقعی ما که قرار است پکت را از router دریافت کنیم خواهد بود و پکت ما محدود به ثابت MTU خواهد بود.
در ادامه بررسی stack ها و روش های دسترسی به استک شبکه ی pfring , xdp , libpcap برای benchmark مورد استفاده قرار گرفت . طی بررسی به نکته ای رسیدم که tshark نیز از libpcap استفاده می کند و هم در مستندات و هم در لینکر به این فیلتر متصل است.
در نهایت با بررسی عملکرد این موارد pcap و pfring عمکلرد مطلوبتری نسبت به tshark و tcpdump داشتند در حد یک عمل ساده ی شمارش پکت ها و لاست کمتری داشتند.

در این بین کمند های زیادی به نسبت بدست آمد که اگر لازم است لیست آنها را بیان کنم در گزارشات بعدی.

از اینجا به بعد صرفا برای مشخص شد درست و یا غلط بودن کارم نحوه benchmark ام را بیان می کنم می توانید نخوانید نحوش قراره صرفا مشخص بشه:

سوالات:
1- مثلا اول wget که سرعت زیاد است آیا مثلا چیزی شبیه burst time داریم. اصلا اینی که گفتم چی هست
2- یا مثلا exhaustion ی چیزی داریم در پورت یا لینک و یا اینترفیس یا شبکه یا هر چیز دیگری که بتونه باشه
3- 

سناریو ها:
1- تست می کنم ببینم اگر سرعت ارسال را کمتر کنیم چقدر کمک می کند به کاهش لاست و باید لاست از بین برود قاعدتا
2- ببینم می شود کاری کرد که با یک Select و یا epoll فقط یک پکت گرفت
3- پکت ها homogen باشد یا heterogen همگنی و غیر همگنی در اینکه بیخودی اول کار فرض کنیم یک بافر بزرگ بدیم دست تابع خواننده موثر است(کلا در این مورد راه رو اشتباه رفتم که این سناریو بذهنم رسید)
4- محاسبه اضافه حتی if اضافه رو حذف کنم
5- نان بلاک بودن سوکت را تست کنم
6- اجرا از داخل ویژوال استادیو و یا از داخل کنسول
7- زمان اجرای برنامه لیستنر مثلا بیشتر باشد یعنی خیلی وقت روی تابع بلاک باشد و بعد آزاد شود یهویی و یا حالت سریع دریافت کن
8- حافظه ارسالی برای پر کردن align باشد و یا نباشد طبق برنامه tst_recvmmsg_flood_udp
9- نکنه ویندوز این وسط گند میزنه به پکت ها
10- نکنه ویرتوال باکس این وسط گند میزنه با نتایج. مشاهده 9 را ببین
11- بافر در هیپ باشه و یا استک
12- برویم سراغ align بودن و میزان الاین بودن روی سرعت دریافت
13- با pcap
14- با pfring
16- با raw packet


مشاهدات:
1- وقتی سرعت ارسال پکت را کم کردم همچنان برنامه من نسخه فقط دریافت یو دی پی لاست داشت متاسفانه . یعنی مثلا فاصله هر پکت را 1 نانو و 10 و 100 و 1000 نانو کردم لاست داشت
2- در برنامه tst_recvmmsg_flood_udp با فاصله اندازی یک میکرون زمان در ارسال به دریافت صد درصدی رسیدیم. در این سناریو ها ارسال یک میلیون واحد بود
3- با برنامه tst_continuly_recvfrom که یعنی دریفات یو دی پی فقط با معطل ماندن توسط تابع recvfrom حتی وقتی فاصله زمانی ارسال 100 نانو ثانیه بود بازهم لاست داشتیم. البته اینجا نان بلاک بود سوکت خواننده
4- سناریوی 3 به همراه بلاک کردن سوکت همچنان لاست داشت. در سناریو های 3 و 4 خواستم همه پی لود را بخوانم
5- در سناریوی 4 به جای اینکه به تابع بگم به اندازه همه بافر حافظه رو بخوت گفتم یک بایت بخون و 100 درصد خوند و لاست نداشت
6- در ادامه سناریوی 5 . حالا همچنان در tst_continuly_recvfrom وقتی ارسال با پی لود تک بایت است و دریافت را دو بایت زدم هم لاست مشاهده شد(اشتباه کرده بودم اینجا)
7- با tst_continuly_recvfrom وقتی دو بایت فرستادم و فاصله زمانی 100 میکرو ثانیه بود و دریافت هم 2 بایت خوند لاست داشتیم
8- در tst_continuly_recvfrom من دو بایت فرستادم ولی یک باید خوندم که لاست داشت
9- با اجرای tst_continuly_recvfrom و ارسال 1 میل فاصله پکت یک وقتایی لاست نداشت و یک وقت هایی لاست داشت. شک کردم به ویندوز. اینجا نمودار یا حس بالاپایین شدن شاید یک چیزایی بگه
10- مشاهده خوب وقتی در tst_continuly_recvfrom پکت یکی بود و آنرا الاین کردم روی 64 بیت و سی پی یو را افینیتی کردم روی صفرمی  و فقط یک بایت خواندم و محاسبات اضافه شامل محاسبه ادرس و این ها رو دیگر انجام ندادم و ارسال تنها با یک نانو ثانیه فاصله انجام شد و Enlarge the receive buffer . که منجر شد دیگر هیچ لاستی نداشتیم. در ضمن اگر فاصله زمانی صفر می شد یعنی ارسال پشت سر هم بود همچنان لاست داشتیم
11- در سناریوی 10 الاین را 8 کردم
12- همه مزیت هایی که در سناریوی 10 بود را حذف کردم غیر از بزرگ کردم بافر رسیو و نتیجه خوب شد و لاست نداشتیم
13- ولی همچنان در سناروی 12 اگر فاصله زمانی ارسال را صفر کنیم بازهم لاست داریم
14- با نسخه tst_continuly_recvfrom حافظه را زیاد کردم. ادرس بافر را الاین کردم. سی پی یو را روی صفر فیکس کردم. غیر بلاکی کردم رید را . تک بایت خواندم . ولی بازهم همه را نخواند در یک میلیون پکت
15- در tst_udp_get_select_recv با سلکت که کار میکردم یک جاهایی سلکت دریافت پکت رو دیگه متوجه نمی شد و در واقع با تایم اوت ان می پرید بیرون
16- در tst_upd_get_epool_recv وقتی با recvfrom , recv  تست کردم همچنان از دریافت و شمارش همه موارد جا ماند و حتی وقتی ارسال کننده تاخیر هم انداخت باز هم نتوانست به آن برسد
17- در سناریوی 16 زمان فاصله بین ارسال را که اوردم روی 10000 نانو ثانیه همه پکت ها دریافت شد
18- در دریافت 10 میلیون پکت تا وقتی سایز 1 و 100 بایت بود یعنی پی لود این مقدار بود tshark خوب عمل کرد و همه را گرفت ولی پی لود که شد 1000 بایت tshark هم لاست داشت
19- در سناریوی testPFRing_count_udp_v1 هم مشاهده شد که اگر 10 میل پکت سایز 1 بایت ارسال بشه پشت هم این لیب می تواند بخواند
20- در سناریوی testPFRing_count_udp_v1 پکت 100 بایت بود و در مقدار 20 میل و این سناریو توانست بگیرد.
21- در سناریوی tst_udp_get_libpcap با اینکه حافظه زیاد کردم ولی مثلا در پی لود 500 بایت لاست داشتیم


نتایج:
1- XDP نشد چون من کارت شبکه  ام ویرتوال است و بعضی امکانات را ندارد و بقول خودش native mode ندارد 